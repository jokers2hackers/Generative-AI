{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHvFXBdxi64i"
      },
      "outputs": [],
      "source": [
        "# Download the stable diffusion model from Hugging Face Diffusers library\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", variant=\"fp16\", dtype=torch.float16, use_auth_token=False).to(\"cuda\")\n",
        "image = pipe(\"An astronaught scuba diving\").images[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from base64 import b64encode\n",
        "import numpy as np\n",
        "from diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# For video display:\n",
        "from IPython.display import HTML\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch import autocast\n",
        "from torchvision import transforms as tfms\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPTextModel, CLIPTokenizer, logging\n",
        "import os\n"
      ],
      "metadata": {
        "id": "rijnFO0GtAlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ~/.cache/huggingface/hub # check if model is cached"
      ],
      "metadata": {
        "id": "C_byuMpPtWsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "if not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()"
      ],
      "metadata": {
        "id": "LyJzPP5ptan_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backens.mps.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "sa3pSIJRtu1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch_device)"
      ],
      "metadata": {
        "id": "DiilWR9Cuk9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the autoencoder model which will be used to decode the latents into image space\n",
        "ae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")"
      ],
      "metadata": {
        "id": "u9EDlHWgumuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer and text encoder to tokenize and encode the text.\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")"
      ],
      "metadata": {
        "id": "-oXvvfu5vPwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the unet model which predicts the amount of noise in the latents\n",
        "\n",
        "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
        "\n",
        "# many options for noise scheduler are available. we go with the  Linear MultiStep Discrete Scheduler (LMSDiscrete).\n",
        "scheduler = LMSDiscreteScheduler(beta_start = 0.00085, beta_end = 0.012, beta_schedule = \"scaled_linear\", num_train_timesteps = 1000)\n",
        "\n"
      ],
      "metadata": {
        "id": "WJH3FOVsvu8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# moving all the models to gpu\n",
        "vae = ae.to(torch_device)\n",
        "text_encoder = text_encoder.to(torch_device)\n",
        "unet = unet.to(torch_device)"
      ],
      "metadata": {
        "id": "yAIfW6UUZTr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# digging a bit deeper into diffusion\n",
        "# Setting params\n",
        "prompt = [\"oil painting of a bull dog\"]\n",
        "height, width = 512, 512 # default resolution of image\n",
        "num_inference_steps = 50 # number of inference steps\n",
        "guidance_scale = 7.5 # the sweet spot to balance image generation diversity and adherence to prompt\n",
        "generator = torch.manual_seed(42) # to replicate results\n",
        "batch_size = 1\n",
        "\n",
        "# get the text embedding for the prompts\n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "\n",
        "# get the text embedding for the unconditional image generation\n",
        "uncond_str = [\"\"] * batch_size\n",
        "uncond_input = tokenizer(uncond_str, padding = \"max_length\", max_length = max_length, return_tensors = \"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
        "\n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings]) # concatenate both unconditional and prompt conditional embeddings\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RGNNQxkXbEGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the noise scheduler\n",
        "\n",
        "scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "# prepare the latents\n",
        "\n",
        "latents = torch.randn(\n",
        "    batch_size, unet.config.in_channels, height // 8, width // 8,\n",
        "    generator = generator\n",
        ") # random noise scaled down to 1/8 th the resolution of original image\n",
        "latents = latents.to(torch_device) # moving to GPU\n",
        "latents = latents * scheduler.init_noise_sigma # scaling the latents\n"
      ],
      "metadata": {
        "id": "HWOADbLdf_Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with autocast(\"cuda\") : # convert to the correct type in cuda (memory efficient and accurate)\n",
        "  for i, t in tqdm(enumerate(scheduler.timesteps), total = len(scheduler.timesteps)):\n",
        "    # expand the latents to the UNet model\n",
        "    latent_model_input = torch.cat([latents] * 2)\n",
        "    # scale the latents - pre-conditioning process\n",
        "    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # predict the noise in the latent space at time step t\n",
        "      noise_pred = unet(latent_model_input, t, encoder_hidden_states = text_embeddings).sample\n",
        "    noise_pred_uncond,noise_pred_text = noise_pred.chunk(2)\n",
        "    # mix them in the required proportion - similar to Kalman filter update\n",
        "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "    # update the latents by removing the predicted noise\n",
        "    latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LQOVabGYiJ3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale and decode the latents back to image\n",
        "latents = 1 / 0.18125 * latents # this particula scale is chosen, because during training, the same scaling is used to account for variance shift\n",
        "with torch.no_grad():\n",
        "  # use a variational auto encoder to retrieve the image from the latent\n",
        "  image = vae.decode(latents).sample\n",
        "\n",
        "# Display the image\n",
        "image = (image / 2 + 0.5).clamp(0,1)\n",
        "# permute to NCHW format and convert to uint8\n",
        "image = image.detach().cpu().permute(0,2,3,1).numpy()\n",
        "image = (image * 255).round().astype(np.uint8)\n",
        "image.shape"
      ],
      "metadata": {
        "id": "6BskC0AodcbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pil_image = [Image.fromarray(img) for img in image]\n",
        "pil_image[0]\n"
      ],
      "metadata": {
        "id": "9DbBMRgAeQjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are able to get a pretty good image of a bull dog in oil painting style with inference_steps set to 50. if we increase the num_inference_steps, there is not much improvement. So, it may not be scalable with respect to the number of inference steps."
      ],
      "metadata": {
        "id": "QCctgchZedb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us dig into the components which make up this inference pipeline - 1. VAE 2. Unet 3. Noise scheduler"
      ],
      "metadata": {
        "id": "QJOuPzPQgyDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Autoencoders help compress an image into a latent space of reduced dimension and then decompress it back to recover the original image\n",
        "\n",
        "def pil_to_latent(input_im) :\n",
        "  # input image shape -> (1, 4, 64, 64)\n",
        "  with torch.no_grad():\n",
        "    latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device) * 2 - 1) # the scaling , this is why we divide by 2 and subtract by 0.5 during decoding\n",
        "  return 0.18125 * latent.latent_dist.sample() # scale the latents by a factor to account for covariance shift during matrix multiplications\n",
        "\n",
        "def latent_to_pil(latent):\n",
        "  # batch of latents -> image\n",
        "  latent = 1/0.18125 * latent\n",
        "  with torch.no_grad():\n",
        "    image = vae.decode(latent).sample\n",
        "  # reverse process of pil_to_latent\n",
        "  image = (image / 2 + 0.5).clamp(0,1)\n",
        "  image = image.detach().cpu().permute(0,2,3,1).numpy()\n",
        "  image = (image * 255).round().astype(np.uint8)\n",
        "  pil_image = [Image.fromarray(img) for img in image]\n",
        "  return pil_image\n"
      ],
      "metadata": {
        "id": "uhOfBwb5ea9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let us download an image from web\n",
        "!pip install bing_image_downloader\n",
        "from bing_image_downloader import downloader\n",
        "downloader.download(query=\"cute kitten\", limit=1, output_dir='.', adult_filter_off=True, force_replace=False, timeout=60)\n"
      ],
      "metadata": {
        "id": "ewR0Uga-icd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = Image.open('/content/cute kitten/Image_1.jpg').resize((512, 512))\n",
        "input_image"
      ],
      "metadata": {
        "id": "G7LS5ISai2Ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the image into latent space\n",
        "latent = pil_to_latent(input_image)\n",
        "print(latent.shape)"
      ],
      "metadata": {
        "id": "hov8nE3OjBjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the latents to get an idea of what the compressed form holds\n",
        "fig, axs = plt.subplots(1, 4, figsize=(16,4))\n",
        "for c in range(4):\n",
        "  axs[c].imshow(latent[0][c].cpu(), cmap = \"Greys\")"
      ],
      "metadata": {
        "id": "5qm0f6d_jLS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the latents capture quite a lot of data about the image like shape, color, textures etc"
      ],
      "metadata": {
        "id": "6Yd7XHgwkrTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let us see if we the decoder gets back the image from these latents\n",
        "reconstructed_image = latent_to_pil(latent)\n",
        "reconstructed_image[0]"
      ],
      "metadata": {
        "id": "sr6avkH5kVBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The diffusion process performed in the latent space and then using the VAE to get the image back saves a lot of computational processes, becausing working with original image resolution is very expensive and takes a lot of time and compute to train. Another approach could be to resize the image to a smaller one, but there could be information loss and blurring effect. This latent space representation through VAE, compresses and also preserves information."
      ],
      "metadata": {
        "id": "nRewc5uVlwAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# playing around with schedulers\n",
        "scheduler.set_timesteps(15)\n",
        "print(scheduler.timesteps)\n",
        "print(scheduler.sigmas)"
      ],
      "metadata": {
        "id": "nJSEWmTplQK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting this noise schedule:\n",
        "plt.plot(scheduler.timesteps)\n",
        "plt.title('Inference sampling timesteps')\n",
        "plt.xlabel('Sampling step')\n",
        "plt.ylabel('Timesteps')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "USaqpXq_QQn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting this noise schedule:\n",
        "plt.plot(scheduler.sigmas)\n",
        "plt.title('Inference sample sigmas')\n",
        "plt.xlabel('Sampling step')\n",
        "plt.ylabel('sigmas')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4VKwRFRvQjgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise = torch.randn_like(latent) # generate a normally distributed noise\n",
        "sampling_step = 10\n",
        "print(scheduler.timesteps[sampling_step], scheduler.sigmas[sampling_step])"
      ],
      "metadata": {
        "id": "moHcm-JTQs5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_noised = scheduler.add_noise(latent, noise, timesteps=torch.tensor([scheduler.timesteps[sampling_step]]))\n",
        "# convert this noised latent to full size image\n",
        "noised_image = latent_to_pil(latent_noised)"
      ],
      "metadata": {
        "id": "17EodfPDSZ06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noised_image[0]"
      ],
      "metadata": {
        "id": "YRTZ47hvS2XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the kitty's structure is somewhat preserved but otherwise the image is very noisy with all the details taken away"
      ],
      "metadata": {
        "id": "r_8bjBPMTQuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let us see how the image looks at different timesteps\n",
        "timesteps = [2, 4, 8, 14]\n",
        "imgs = []\n",
        "for t in timesteps:\n",
        "  noise = torch.randn_like(latent)\n",
        "  sampling_step = t\n",
        "  latent_noised = scheduler.add_noise(latent, noise, timesteps=torch.tensor([scheduler.timesteps[sampling_step]]))\n",
        "  # convert this noised latent to full size image\n",
        "  noised_image = latent_to_pil(latent_noised)\n",
        "  imgs.append(noised_image[0])\n"
      ],
      "metadata": {
        "id": "dzmLt4sBTOwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 4, figsize=(16,4))\n",
        "for i in range(len(timesteps)):\n",
        "  axs[i].imshow(imgs[i])"
      ],
      "metadata": {
        "id": "1GFJ4triT67l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what does add_noise do?\n",
        "??scheduler.add_noise"
      ],
      "metadata": {
        "id": "d2e3n3_qUPAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During inference, we don't start with a noisy image. instead we have a noisy latent generated by scaling it with the largest variance, hence the factor 0.18125 in the code.this is particulary applicable for this type of model with this scheduler. if a model uses different scheduler , these constants change"
      ],
      "metadata": {
        "id": "3CpX1OsKVN7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have already seen how to add noise to images through latents and remove noise and get back the image as well. So, it is easier to understand image2image generation. Given an image and a text promot, the diffusion model can generate an image adhering to the text prompt. Instead of starting from noisy latents, we start with reference image"
      ],
      "metadata": {
        "id": "1Q0vyqC1W4_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"A kitten with a hat and a sunglass on\"\n",
        "height , width = 512, 512\n",
        "num_inference_steps = 50\n",
        "guidance_scale = 7.5 # usually works well, but can be tuned\n",
        "generator = torch.manual_seed(42)\n",
        "batch_size = 1\n",
        "\n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length = tokenizer.model_max_length,\n",
        "                       truncation=True, return_tensors = \"pt\")\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_text = [\"\"] * batch_size\n",
        "uncond_input = tokenizer(uncond_text, padding=\"max_length\", max_length = max_length, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UCGrN8QvUxIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler.set_timesteps(num_inference_steps)"
      ],
      "metadata": {
        "id": "vdTD7jA7Yl-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare latents from the already existing latent of the cute kitten we have from before\n",
        "start_step = 10\n",
        "start_sigma = scheduler.sigmas[start_step]\n",
        "noise = torch.randn_like(latent)\n",
        "new_latent = scheduler.add_noise(latent, noise, torch.tensor([scheduler.timesteps[start_step]]))\n",
        "new_latent = new_latent.to(torch_device).float()"
      ],
      "metadata": {
        "id": "2R8YeGtXYsmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, t in tqdm(enumerate(scheduler.timesteps), total = len(scheduler.timesteps)):\n",
        "  if i >= start_step: # we already have added noise of the first 10 steps to the image's latent\n",
        "    latent_model_input = torch.cat([new_latent] * 2)\n",
        "    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "    # predict the noise at the timestep t\n",
        "    with torch.no_grad():\n",
        "      noise_pred = unet(latent_model_input, t, encoder_hidden_states = text_embeddings).sample\n",
        "    # split text and unconditional noise\n",
        "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "    # perform classifier free guidance\n",
        "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "    # compute the previously noisy sample x_t-1 given x_t\n",
        "    new_latent = scheduler.step(noise_pred, t, new_latent).prev_sample\n",
        "\n"
      ],
      "metadata": {
        "id": "51v9waQ1ZHEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_image = latent_to_pil(new_latent)"
      ],
      "metadata": {
        "id": "ME454xl4atPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_image[0]"
      ],
      "metadata": {
        "id": "d4JiUI14a1IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "not bad, with a request for hat and sunglass, the model generates an image with sunglass on but missing hat, maybe a higher weightage to guidance scale may improve the result"
      ],
      "metadata": {
        "id": "PsLsG131a_q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let us put the code in a function to play around with different guidance scale and start step\n",
        "\n",
        "def img2img(guidance_scale, start_step):\n",
        "  prompt = \"A kitten with a hat and a sunglass on\"\n",
        "  height , width = 512, 512\n",
        "  num_inference_steps = 50\n",
        "  guidance_scale = 7.5 # usually works well, but can be tuned\n",
        "  generator = torch.manual_seed(42)\n",
        "  batch_size = 1\n",
        "\n",
        "  text_input = tokenizer(prompt, padding=\"max_length\", max_length = tokenizer.model_max_length,\n",
        "                        truncation=True, return_tensors = \"pt\")\n",
        "  with torch.no_grad():\n",
        "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "  max_length = text_input.input_ids.shape[-1]\n",
        "  uncond_text = [\"\"] * batch_size\n",
        "  uncond_input = tokenizer(uncond_text, padding=\"max_length\", max_length = max_length, return_tensors=\"pt\")\n",
        "  with torch.no_grad():\n",
        "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
        "  text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "  scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "  start_sigma = scheduler.sigmas[start_step]\n",
        "  noise = torch.randn_like(latent)\n",
        "  new_latent = scheduler.add_noise(latent, noise, torch.tensor([scheduler.timesteps[start_step]]))\n",
        "  new_latent = new_latent.to(torch_device).float()\n",
        "\n",
        "  for i, t in tqdm(enumerate(scheduler.timesteps), total = len(scheduler.timesteps)):\n",
        "    if i >= start_step: # we already have added noise of the first 10 steps to the image's latent\n",
        "      latent_model_input = torch.cat([new_latent] * 2)\n",
        "      latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "      # predict the noise at the timestep t\n",
        "      with torch.no_grad():\n",
        "        noise_pred = unet(latent_model_input, t, encoder_hidden_states = text_embeddings).sample\n",
        "      # split text and unconditional noise\n",
        "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "      # perform classifier free guidance\n",
        "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "      # compute the previously noisy sample x_t-1 given x_t\n",
        "      new_latent = scheduler.step(noise_pred, t, new_latent).prev_sample\n",
        "  new_image = latent_to_pil(new_latent)\n",
        "  return new_image[0]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dd6EIH2Ka14J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img2img(7.5, 15)"
      ],
      "metadata": {
        "id": "PYSPnap8b_Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "increase in start step worsens the result with respect to adherence of prompt. So, let us decrease the start_step,. this could move the image away from original image, but worth experimenting"
      ],
      "metadata": {
        "id": "r5_fC7w1cJ-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img2img(7.5, 5)"
      ],
      "metadata": {
        "id": "nqKdWTh5cBsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is better with respect to adherence to prompt, but as expected, the image has deviated from the original image. let us now increase the guidance scale"
      ],
      "metadata": {
        "id": "ra7nxCQrcfn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img2img(10, 5)"
      ],
      "metadata": {
        "id": "J529zRofcXMI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}